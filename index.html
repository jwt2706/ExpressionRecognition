<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Image Classification</title>
    <style>
        #loadingIndicator {
            display: none;
            font-size: 18px;
            color: #ff5733;
        }
    </style>
    <script type="module">
        import { pipeline } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers/dist/transformers.min.js';

        let classifier;
        let classificationInterval;
        let isCameraOn = false; // track camera state
        let stream; // store the video stream

        // Load sound effects
        const sounds = {
            happy: new Audio('/sounds/heheheha.mp3'),
            sad: new Audio('/sounds/ehohoui.mp3'),
            angry: new Audio('sounds/grrr.mp3')
        };

        async function setupClassifier() {
            document.getElementById('loadingIndicator').style.display = 'block'; // show loading indicator if model isn't ready yet
            classifier = await pipeline('image-classification', 'Xenova/facial_emotions_image_detection');
            document.getElementById('loadingIndicator').style.display = 'none'; // hide loading indicator when model is ready
            startCamera(); // start the camera after model is loaded
        }

        async function classifyImage() {
            const canvas = document.createElement('canvas');
            const video = document.getElementById('video');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;

            const context = canvas.getContext('2d');
            context.drawImage(video, 0, 0, canvas.width, canvas.height);
            const imageDataUrl = canvas.toDataURL('image/png');

            const output = await classifier(imageDataUrl);

            const formattedOutput = output.map(emotion => {
                return `${emotion.label}: ${(emotion.score * 100).toFixed(2)}%`;
            }).join('\n');

            document.getElementById('output').textContent = formattedOutput;
            playSound(output); // play sound based on detected emotions
        }

        function playSound(output) {
            // Stop all sounds first
            for (const sound of Object.values(sounds)) {
                sound.pause();
                sound.currentTime = 0; // Reset to start
            }

            // Play sound for the detected emotion with the highest score
            const highestEmotion = output.reduce((prev, current) => (prev.score > current.score) ? prev : current);

            switch (highestEmotion.label) {
                case 'happy':
                    sounds.happy.play();
                    break;
                case 'sad':
                    sounds.sad.play();
                    break;
                case 'angry':
                    sounds.angry.play();
                    break;
                default:
                    break; // Other emotions do not play sound
            }
        }

        async function startCamera() {
            const video = document.getElementById('video');
            if (!isCameraOn) {
                try {
                    stream = await navigator.mediaDevices.getUserMedia({ video: true });
                    video.srcObject = stream;
                    video.play();
                    
                    const delay = parseFloat(document.getElementById('delaySlider').value) * 1000;
                    classificationInterval = setInterval(classifyImage, delay);
                    isCameraOn = true; // update camera state
                } catch (error) {
                    alert('Camera could not be found. Please ensure it is connected and try again.');
                    console.error('Error accessing the camera:', error);
                }
            }
        }

        function updateInterval() {
            if (isCameraOn) {
                stopClassification(); // stop the current interval
                const delay = parseFloat(document.getElementById('delaySlider').value) * 1000; // convert to milliseconds
                classificationInterval = setInterval(classifyImage, delay); // start a new interval with the updated delay
            }
        }

        function stopClassification() {
            if (classificationInterval) {
                clearInterval(classificationInterval);
                classificationInterval = null;
            }
        }

        document.addEventListener('DOMContentLoaded', async () => {
            await setupClassifier();
            document.getElementById('delaySlider').addEventListener('input', updateInterval);
        });
    </script>
</head>
<body>
    <h1>Facial Emotion Recognition</h1>
    <div id="loadingIndicator">Loading model, please wait... (this can take a minute)</div>
    <video id="video" width="300" autoplay></video>
    
    <label for="delaySlider">Delay (seconds): </label>
    <input id="delaySlider" type="range" min="0.2" max="5" step="0.1" value="1">
    <span id="delayValue">1</span> seconds

    <pre id="output"></pre>

    <script>
        // update displayed value of the slider
        const delaySlider = document.getElementById('delaySlider');
        const delayValue = document.getElementById('delayValue');
        delaySlider.addEventListener('input', () => {
            delayValue.textContent = delaySlider.value;
        });
    </script>
</body>
</html>
